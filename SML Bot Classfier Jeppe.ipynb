{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: IPython in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (8.25.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.32.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (4.12.3)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (1.50.2)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (2.5.3)\n",
      "Requirement already satisfied: python-louvain in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.16)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2.0.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (2.2.2)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (3.8.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (3.2.1)\n",
      "Requirement already satisfied: community in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (1.0.0b1)\n",
      "Requirement already satisfied: setfit in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (1.1.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (1.4.2)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (2.4.1)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (4.45.1)\n",
      "Requirement already satisfied: gradio in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (4.44.0)\n",
      "Collecting pyvis (from -r requirements.txt (line 20))\n",
      "  Using cached pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (3.7.6)\n",
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (2.1.1)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from IPython->-r requirements.txt (line 1)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from IPython->-r requirements.txt (line 1)) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from IPython->-r requirements.txt (line 1)) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from IPython->-r requirements.txt (line 1)) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from IPython->-r requirements.txt (line 1)) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from IPython->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from IPython->-r requirements.txt (line 1)) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from IPython->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->-r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->-r requirements.txt (line 2)) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->-r requirements.txt (line 3)) (2.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai->-r requirements.txt (line 4)) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai->-r requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai->-r requirements.txt (line 4)) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai->-r requirements.txt (line 4)) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai->-r requirements.txt (line 4)) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai->-r requirements.txt (line 4)) (4.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic->-r requirements.txt (line 5)) (2.14.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 8)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 8)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 10)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 10)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 10)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 10)) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 10)) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 10)) (3.0.9)\n",
      "Requirement already satisfied: Flask in /opt/anaconda3/lib/python3.12/site-packages (from community->-r requirements.txt (line 13)) (3.0.3)\n",
      "Requirement already satisfied: sentence-transformers>=3 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers[train]>=3->setfit->-r requirements.txt (line 14)) (3.1.1)\n",
      "Requirement already satisfied: evaluate>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from setfit->-r requirements.txt (line 14)) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from setfit->-r requirements.txt (line 14)) (0.24.6)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 15)) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 15)) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 15)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 15)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 15)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->-r requirements.txt (line 15)) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 15)) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 15)) (6.0.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 16)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 16)) (2.2.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 17)) (1.12)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 17)) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch->-r requirements.txt (line 17)) (69.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 18)) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 18)) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 18)) (0.20.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (23.2.1)\n",
      "Requirement already satisfied: fastapi<1.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (0.115.0)\n",
      "Requirement already satisfied: ffmpy in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (1.3.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (6.4.5)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (3.10.7)\n",
      "Requirement already satisfied: pydub in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (0.0.10)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (0.6.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (0.12.5)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio->-r requirements.txt (line 19)) (0.30.6)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /opt/anaconda3/lib/python3.12/site-packages (from gradio-client==1.3.0->gradio->-r requirements.txt (line 19)) (12.0)\n",
      "Collecting jsonpickle>=1.4.1 (from pyvis->-r requirements.txt (line 20))\n",
      "  Using cached jsonpickle-4.0.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 21)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 21)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 21)) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 21)) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 21)) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 21)) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 21)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 21)) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 21)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 21)) (0.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 21)) (3.4.1)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /opt/anaconda3/lib/python3.12/site-packages (from fastapi<1.0->gradio->-r requirements.txt (line 19)) (0.38.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 15)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 15)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 15)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 15)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 15)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 15)) (1.15.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->IPython->-r requirements.txt (line 1)) (0.8.3)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 21)) (1.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->IPython->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->-r requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers[train]>=3->setfit->-r requirements.txt (line 14)) (0.34.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 21)) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 21)) (0.1.5)\n",
      "Collecting numpy (from -r requirements.txt (line 7))\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 19)) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 19)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 19)) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 21)) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 21)) (5.2.1)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from Flask->community->-r requirements.txt (line 13)) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from Flask->community->-r requirements.txt (line 13)) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/anaconda3/lib/python3.12/site-packages (from Flask->community->-r requirements.txt (line 13)) (1.6.2)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->IPython->-r requirements.txt (line 1)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->IPython->-r requirements.txt (line 1)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->IPython->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch->-r requirements.txt (line 17)) (1.3.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.20.3->sentence-transformers[train]>=3->setfit->-r requirements.txt (line 14)) (5.9.0)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 21)) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 19)) (2.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->-r requirements.txt (line 15)) (0.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 19)) (0.1.0)\n",
      "Using cached pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
      "Using cached jsonpickle-4.0.0-py3-none-any.whl (46 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Installing collected packages: numpy, jsonpickle, pyvis\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.0\n",
      "    Uninstalling numpy-2.0.0:\n",
      "      Successfully uninstalled numpy-2.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-profiling 3.2.0 requires joblib~=1.1.0, but you have joblib 1.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed jsonpickle-4.0.0 numpy-1.26.4 pyvis-0.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datahandling\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import \n",
    "data = pd.read_csv('detection_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Retweet Count</th>\n",
       "      <th>Mention Count</th>\n",
       "      <th>Follower Count</th>\n",
       "      <th>Verified</th>\n",
       "      <th>Bot Label</th>\n",
       "      <th>Location</th>\n",
       "      <th>Created At</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>URLs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132131</td>\n",
       "      <td>flong</td>\n",
       "      <td>Station activity person against natural majori...</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>2353</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Adkinston</td>\n",
       "      <td>2020-05-11 15:29:50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>289683</td>\n",
       "      <td>hinesstephanie</td>\n",
       "      <td>Authority research natural life material staff...</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>9617</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Sanderston</td>\n",
       "      <td>2022-11-26 05:18:10</td>\n",
       "      <td>both live</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>779715</td>\n",
       "      <td>roberttran</td>\n",
       "      <td>Manage whose quickly especially foot none to g...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4363</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Harrisonfurt</td>\n",
       "      <td>2022-08-08 03:16:54</td>\n",
       "      <td>phone ahead</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>696168</td>\n",
       "      <td>pmason</td>\n",
       "      <td>Just cover eight opportunity strong policy which.</td>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>2242</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Martinezberg</td>\n",
       "      <td>2021-08-14 22:27:05</td>\n",
       "      <td>ever quickly new I</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704441</td>\n",
       "      <td>noah87</td>\n",
       "      <td>Animal sign six data good or.</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>8438</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Camachoville</td>\n",
       "      <td>2020-04-13 21:24:21</td>\n",
       "      <td>foreign mention</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>570928</td>\n",
       "      <td>james00</td>\n",
       "      <td>See wonder travel this suffer less yard office...</td>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>3792</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>West Cheyenne</td>\n",
       "      <td>2023-05-07 22:24:47</td>\n",
       "      <td>anyone respond perhaps market run</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>734182</td>\n",
       "      <td>leonard00</td>\n",
       "      <td>Door final sound my guess building rich.</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>South Donald</td>\n",
       "      <td>2021-01-21 03:02:53</td>\n",
       "      <td>president</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>107312</td>\n",
       "      <td>lesterdaniel</td>\n",
       "      <td>Job phone price magazine worry stage check view.</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>1442</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Smithhaven</td>\n",
       "      <td>2022-06-12 16:45:16</td>\n",
       "      <td>option husband admit</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>549888</td>\n",
       "      <td>kimberlymorris</td>\n",
       "      <td>Eye rest prove mission show floor.</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>836</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Lake Brittanyville</td>\n",
       "      <td>2021-12-19 19:00:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>117640</td>\n",
       "      <td>schmittjill</td>\n",
       "      <td>Add letter year performance western what cultu...</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>6523</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>West Hannahborough</td>\n",
       "      <td>2022-07-30 05:39:16</td>\n",
       "      <td>available thing</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>576805</td>\n",
       "      <td>hicksanthony</td>\n",
       "      <td>Through both practice point fear billion deep.</td>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>8694</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Harrisbury</td>\n",
       "      <td>2020-08-30 10:04:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>758550</td>\n",
       "      <td>hooperdennis</td>\n",
       "      <td>Wonder accept edge enjoy top organization tech...</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>5986</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Rossmouth</td>\n",
       "      <td>2021-03-12 23:54:15</td>\n",
       "      <td>treat care eat author</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>383629</td>\n",
       "      <td>rcruz</td>\n",
       "      <td>Recognize nor choose over trade another involve.</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>6779</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Juliehaven</td>\n",
       "      <td>2021-06-13 22:03:42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>850918</td>\n",
       "      <td>dstewart</td>\n",
       "      <td>Them relate question religious require popular.</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>6073</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Port Jessicaborough</td>\n",
       "      <td>2023-05-13 20:43:12</td>\n",
       "      <td>upon all direction fly</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>675134</td>\n",
       "      <td>tracibell</td>\n",
       "      <td>Reality foreign land front PM trip him.</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>4846</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Susanville</td>\n",
       "      <td>2020-11-16 23:10:39</td>\n",
       "      <td>course difficult fine</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    User ID        Username  \\\n",
       "0    132131           flong   \n",
       "1    289683  hinesstephanie   \n",
       "2    779715      roberttran   \n",
       "3    696168          pmason   \n",
       "4    704441          noah87   \n",
       "5    570928         james00   \n",
       "6    734182       leonard00   \n",
       "7    107312    lesterdaniel   \n",
       "8    549888  kimberlymorris   \n",
       "9    117640     schmittjill   \n",
       "10   576805    hicksanthony   \n",
       "11   758550    hooperdennis   \n",
       "12   383629           rcruz   \n",
       "13   850918        dstewart   \n",
       "14   675134       tracibell   \n",
       "\n",
       "                                                Tweet  Retweet Count  \\\n",
       "0   Station activity person against natural majori...             85   \n",
       "1   Authority research natural life material staff...             55   \n",
       "2   Manage whose quickly especially foot none to g...              6   \n",
       "3   Just cover eight opportunity strong policy which.             54   \n",
       "4                       Animal sign six data good or.             26   \n",
       "5   See wonder travel this suffer less yard office...             41   \n",
       "6            Door final sound my guess building rich.             54   \n",
       "7    Job phone price magazine worry stage check view.             64   \n",
       "8                  Eye rest prove mission show floor.             25   \n",
       "9   Add letter year performance western what cultu...             67   \n",
       "10     Through both practice point fear billion deep.             57   \n",
       "11  Wonder accept edge enjoy top organization tech...             29   \n",
       "12   Recognize nor choose over trade another involve.             60   \n",
       "13    Them relate question religious require popular.             61   \n",
       "14            Reality foreign land front PM trip him.             21   \n",
       "\n",
       "    Mention Count  Follower Count  Verified  Bot Label             Location  \\\n",
       "0               1            2353     False          1            Adkinston   \n",
       "1               5            9617      True          0           Sanderston   \n",
       "2               2            4363      True          0         Harrisonfurt   \n",
       "3               5            2242      True          1         Martinezberg   \n",
       "4               3            8438     False          1         Camachoville   \n",
       "5               4            3792      True          1        West Cheyenne   \n",
       "6               0              10      True          0         South Donald   \n",
       "7               0            1442     False          1           Smithhaven   \n",
       "8               2             836     False          0   Lake Brittanyville   \n",
       "9               3            6523     False          1   West Hannahborough   \n",
       "10              4            8694     False          1           Harrisbury   \n",
       "11              1            5986      True          1            Rossmouth   \n",
       "12              2            6779     False          0           Juliehaven   \n",
       "13              0            6073      True          0  Port Jessicaborough   \n",
       "14              2            4846     False          0           Susanville   \n",
       "\n",
       "             Created At                           Hashtags URLs  \n",
       "0   2020-05-11 15:29:50                                NaN   []  \n",
       "1   2022-11-26 05:18:10                          both live   []  \n",
       "2   2022-08-08 03:16:54                        phone ahead   []  \n",
       "3   2021-08-14 22:27:05                 ever quickly new I   []  \n",
       "4   2020-04-13 21:24:21                    foreign mention   []  \n",
       "5   2023-05-07 22:24:47  anyone respond perhaps market run   []  \n",
       "6   2021-01-21 03:02:53                          president   []  \n",
       "7   2022-06-12 16:45:16               option husband admit   []  \n",
       "8   2021-12-19 19:00:16                                NaN   []  \n",
       "9   2022-07-30 05:39:16                    available thing   []  \n",
       "10  2020-08-30 10:04:51                                NaN   []  \n",
       "11  2021-03-12 23:54:15              treat care eat author   []  \n",
       "12  2021-06-13 22:03:42                                NaN   []  \n",
       "13  2023-05-13 20:43:12             upon all direction fly   []  \n",
       "14  2020-11-16 23:10:39              course difficult fine   []  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_urls(tweet):\n",
    "    # Regulært udtryk til at finde URLs\n",
    "    url_pattern = r'https?://\\S+'\n",
    "    \n",
    "    # Find alle matchende URLs i tweet'en\n",
    "    urls = re.findall(url_pattern, tweet)\n",
    "    \n",
    "    return urls\n",
    "# Tilføj en ny kolonne med URLs\n",
    "data['URLs'] = data['Tweet'].apply(extract_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Station activity person against natural majori...\n",
      "1    Authority research natural life material staff...\n",
      "2    Manage whose quickly especially foot none to g...\n",
      "3    Just cover eight opportunity strong policy which.\n",
      "4                        Animal sign six data good or.\n",
      "Name: Tweet, dtype: object\n",
      "0    []\n",
      "1    []\n",
      "2    []\n",
      "3    []\n",
      "4    []\n",
      "Name: URLs, dtype: object\n",
      "Antal tweets med URLs: 0\n"
     ]
    }
   ],
   "source": [
    "# Tjek indholdet af tweets\n",
    "print(data['Tweet'].head())\n",
    "# Mere robust URL-extraction\n",
    "import re\n",
    "def extract_urls(tweet):\n",
    "    # Mere omfattende URL-pattern\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    \n",
    "    urls = re.findall(url_pattern, str(tweet))\n",
    "    return urls\n",
    "# Anvend ny metode\n",
    "data['URLs'] = data['Tweet'].apply(extract_urls)\n",
    "# Tjek resultat\n",
    "print(data['URLs'].head())\n",
    "print(f\"Antal tweets med URLs: {data['URLs'].apply(len).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Antager at du allerede har importeret datasættet som 'data'\n",
    "# Hvis datasættet er importeret som 'data', kan du fortsætte direkte fra her\n",
    "\n",
    "# Funktion til at finde URL'er i tekst\n",
    "# Opdateret regex for at finde mere generelle URL'er\n",
    "def find_urls(text):\n",
    "    # Regular expression til at finde URL'er, som kan inkludere både http(s) og uden http\n",
    "    url_pattern = r'(\\b(?:https?://|www\\.)[a-zA-Z0-9-_.]+\\.[a-zA-Z]{2,}\\b(?:[^\\s]*))'\n",
    "    return re.findall(url_pattern, text)\n",
    "\n",
    "# Find alle URL'er i Tweet-kolonnen\n",
    "urls = data['Tweet'].apply(find_urls)\n",
    "\n",
    "# Saml alle URL'er i en liste\n",
    "all_urls = [url for sublist in urls for url in sublist]\n",
    "\n",
    "# Fjern duplikater\n",
    "unique_urls = list(set(all_urls))\n",
    "\n",
    "# Udskriv listen med unikke URL'er\n",
    "print(unique_urls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"\\nBasic information about the dataset:\")\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Hashtags'] = data['Hashtags'].fillna('<missing>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Preparation\n",
    "# Antag at \"data\" er din dataframe\n",
    "X = data.drop(columns=['Bot Label'])  # Features\n",
    "y = data['Bot Label']  # Target\n",
    "\n",
    "# Håndtering af manglende værdier og tidsfunktioner\n",
    "X['Hashtags'] = X['Hashtags'].fillna('<missing>')\n",
    "X['Created At'] = pd.to_datetime(X['Created At'])\n",
    "X['Year'] = X['Created At'].dt.year\n",
    "X['Month'] = X['Created At'].dt.month\n",
    "X['Hour'] = X['Created At'].dt.hour\n",
    "\n",
    "# Drop User ID og Created At (efter at have udledt funktioner)\n",
    "X = X.drop(columns=['User ID', 'Created At'])\n",
    "\n",
    "# Identificer kategoriske og numeriske kolonner\n",
    "categorical_cols = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Skaler numeriske kolonner\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Kolonnetransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split datasæt\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Feature selection og modeltræning\n",
    "def select_features(X_train, y_train, X_val, k=10):\n",
    "    # Transformér data\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_val_transformed = preprocessor.transform(X_val)\n",
    "    \n",
    "    # Brug f_classif i stedet for chi2 for at undgå problemer med negative værdier\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_selected = selector.fit_transform(X_train_transformed, y_train)\n",
    "    X_val_selected = selector.transform(X_val_transformed)\n",
    "    return X_train_selected, X_val_selected, selector\n",
    "\n",
    "# Initialiser modeller inkl. XGBoost\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": xgb.XGBClassifier(eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    X_train_selected, X_val_selected, selector = select_features(X_train, y_train, X_val, k=10)\n",
    "    \n",
    "    # Hyperparameter tuning for Random Forest og XGBoost\n",
    "    if name == \"Random Forest\":\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [None, 10, 20]\n",
    "        }\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "        model = grid_search.best_estimator_\n",
    "    elif name == \"XGBoost\":\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2]\n",
    "        }\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "        model = grid_search.best_estimator_\n",
    "    else:\n",
    "        model.fit(X_train_selected, y_train)\n",
    "\n",
    "    y_val_pred = model.predict(X_val_selected)\n",
    "    y_val_proba = model.predict_proba(X_val_selected)[:, 1] if hasattr(model, 'predict_proba') else y_val_pred\n",
    "\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy_score(y_val, y_val_pred),\n",
    "        \"Precision\": precision_score(y_val, y_val_pred),\n",
    "        \"Recall\": recall_score(y_val, y_val_pred),\n",
    "        \"F1 Score\": f1_score(y_val, y_val_pred),\n",
    "        \"ROC AUC\": roc_auc_score(y_val, y_val_proba)\n",
    "    }\n",
    "\n",
    "# Evaluér på test-sæt\n",
    "best_model_name = max(results, key=lambda k: results[k]['Accuracy'])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "X_test_selected = selector.transform(X_test_transformed)\n",
    "y_test_pred = best_model.predict(X_test_selected)\n",
    "y_test_proba = best_model.predict_proba(X_test_selected)[:, 1] if hasattr(best_model, 'predict_proba') else y_test_pred\n",
    "\n",
    "# Udskriv resultater\n",
    "print(\"\\nFinal Evaluation on Test Set:\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(f\"Test Set ROC AUC: {roc_auc_score(y_test, y_test_proba):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(n=2000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Preparation\n",
    "X = data.drop(columns=['Bot Label'])  # Features\n",
    "y = data['Bot Label']  # Target\n",
    "\n",
    "# Håndtering af manglende værdier og tidsfunktioner\n",
    "X['Hashtags'] = X['Hashtags'].fillna('<missing>')\n",
    "X['Created At'] = pd.to_datetime(X['Created At'])\n",
    "X['Year'] = X['Created At'].dt.year\n",
    "X['Month'] = X['Created At'].dt.month\n",
    "X['Hour'] = X['Created At'].dt.hour\n",
    "\n",
    "# Drop User ID og Created At (efter at have udledt funktioner)\n",
    "X = X.drop(columns=['User ID', 'Created At'])\n",
    "\n",
    "# Identificer kategoriske kolonner\n",
    "categorical_cols = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Skaler numeriske kolonner\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Kolonnetransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split datasæt\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialiser modeller\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight='balanced'),\n",
    "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "results = {}\n",
    "trained_models = {}  # Dictionary to store the actual trained models\n",
    "\n",
    "# Training & Hyperparameter tuning\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "\n",
    "    # Transform features\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_val_transformed = preprocessor.transform(X_val)\n",
    "\n",
    "    # Hyperparameter tuning for Random Forest and XGBoost\n",
    "    if name == \"Random Forest\":\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "        grid_search.fit(X_train_transformed, y_train)\n",
    "        trained_models[name] = grid_search.best_estimator_\n",
    "    elif name == \"XGBoost\":\n",
    "        param_grid_xgb = {\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0]\n",
    "        }\n",
    "        grid_search_xgb = GridSearchCV(model, param_grid_xgb, cv=5, scoring='accuracy')\n",
    "        grid_search_xgb.fit(X_train_transformed, y_train)\n",
    "        trained_models[name] = grid_search_xgb.best_estimator_\n",
    "    else:\n",
    "        model.fit(X_train_transformed, y_train)\n",
    "        trained_models[name] = model\n",
    "\n",
    "    # Store model evaluation metrics\n",
    "    y_val_pred = trained_models[name].predict(X_val_transformed)\n",
    "    y_val_proba = trained_models[name].predict_proba(X_val_transformed)[:, 1] if hasattr(trained_models[name], 'predict_proba') else y_val_pred\n",
    "\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy_score(y_val, y_val_pred),\n",
    "        \"Precision\": precision_score(y_val, y_val_pred),\n",
    "        \"Recall\": recall_score(y_val, y_val_pred),\n",
    "        \"F1 Score\": f1_score(y_val, y_val_pred),\n",
    "        \"ROC AUC\": roc_auc_score(y_val, y_val_proba)\n",
    "    }\n",
    "\n",
    "# Evaluér på test-sæt\n",
    "best_model_name = max(results, key=lambda k: results[k]['Accuracy'])\n",
    "best_model = trained_models[best_model_name]  # Retrieve the actual best model\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "y_test_pred = best_model.predict(X_test_transformed)\n",
    "y_test_proba = best_model.predict_proba(X_test_transformed)[:, 1] if hasattr(best_model, 'predict_proba') else y_test_pred\n",
    "\n",
    "print(\"\\nFinal Evaluation on Test Set:\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(f\"Test Set ROC AUC: {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
    "\n",
    "# Print results for all models\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    for metric, score in metrics.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
